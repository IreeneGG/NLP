text	summary
La inteligencia artificial (abreviado: IA), en el contexto de las ciencias de la computación, es una disciplina y un conjunto de capacidades cognoscitivas e intelectuales expresadas por sistemas informáticos o combinaciones de algoritmos cuyo propósito es la creación de máquinas que imiten la inteligencia humana para realizar tareas, y que pueden mejorar conforme recopilen información. Estas tecnologías permiten que las máquinas aprendan de la experiencia, se adapten a nuevas entradas y realicen tareas humanas como el reconocimiento de voz, la toma de decisiones, la traducción de idiomas o la visión por computadora.​​  En la actualidad, la inteligencia artificial abarca una gran variedad de subcampos. Éstos van desde áreas de propósito general, aprendizaje y percepción, a otras más específicas como el reconocimiento de voz, el juego de ajedrez, la demostración de teoremas matemáticos, la escritura de poesía y el diagnóstico de enfermedades. La inteligencia artificial sintetiza y automatiza tareas que en principio son intelectuales y, por lo tanto, es potencialmente relevante para cualquier ámbito de actividades intelectuales humanas. En este sentido, es un campo genuinamente universal, además, la IA se encuentra en constante evolución gracias al desarrollo de tecnologías como el aprendizaje profundo, redes neuronales y procesamiento del lenguaje natural, lo cual permite un avance acelerado en su capacidad para resolver problemas complejos.​ La arquitectura de las inteligencias artificiales y los procesos por los cuales aprenden, se mejoran y se implementan en algún área de interés que varía según el enfoque de utilidad que se les quiera dar, pero de manera general, estos van desde la ejecución de sencillos algoritmos hasta la interconexión de complejas redes neuronales artificiales que intentan replicar los circuitos neuronales del cerebro humano y que aprenden mediante diferentes modelos de aprendizaje tales como el aprendizaje automático, el aprendizaje por refuerzo, el aprendizaje profundo y el aprendizaje supervisado.​ Por otro lado, el desarrollo y aplicación de la inteligencia artificial en muchos aspectos de la vida cotidiana también ha propiciado la creación de nuevos campos de estudio como la roboética y la ética de las máquinas, que abordan aspectos relacionados con la ética en la inteligencia artificial y que se encargan de analizar cómo los avances en este tipo de tecnologías impactarían en diversos ámbitos de la vida, así como el manejo responsable y ético que se les debería dar a los mismos, además de establecer cuál debería ser la manera correcta de proceder de las máquinas y las reglas que deberían cumplir.​ En cuanto a su clasificación, tradicionalmente se divide a la inteligencia artificial en inteligencia artificial débil, la cual es la única que existe en la actualidad y que se ocupa de realizar tareas específicas, e inteligencia artificial general, que sería una IA que excediese las capacidades humanas. Algunos expertos creen que si alguna vez se alcanza este nivel, se podría dar lugar a la aparición de una singularidad tecnológica, es decir, una entidad tecnológica superior que se mejoraría a sí misma constantemente, volviéndose incontrolable para los humanos, dando pie a teorías como el basilisco de Roko.​ Algunas de las inteligencias artificiales más conocidas y utilizadas en la actualidad alrededor del mundo incluyen inteligencia artificial en el campo de la salud, asistentes virtuales como Alexa, el asistente de Google o Siri, traductores automáticos como el traductor de Google y DeepL, sistemas de recomendación como el de la plataforma digital de YouTube, motores de ajedrez y otros juegos como Stockfish y AlphaZero, chatbots como ChatGPT, creadores de arte de inteligencia artificial como Midjourney, Dall-e, Leonardo y Stable Diffusion, e incluso la conducción de vehículos autónomos como Tesla Autopilot.​ En 2019 la Comisión Mundial de Ética del Conocimiento	La inteligencia artificial (IA) es una rama de la informática que crea sistemas capaces de imitar la inteligencia humana para realizar diversas tareas, aprendiendo de la experiencia y adaptándose a nuevas informaciones.  Actualmente, la IA abarca numerosos campos, desde el reconocimiento de voz hasta la conducción autónoma,  generando a su vez debates éticos sobre su desarrollo y uso responsable.
En el aprendizaje automático, una red neuronal artificial (abreviada ANN o NN) es un modelo dentro de los llamados sistemas conexionistas, inspirado en la estructura y función de las redes neuronales biológicas en los cerebros animales. Una ANN consta de unidades o nodos conectados llamados neuronas artificiales, que modelan vagamente las neuronas del cerebro. Estas están conectadas por bordes, que modelan las sinapsis del cerebro. Cada neurona artificial recibe «señales» de las neuronas conectadas, luego las procesa y envía una señal a otras neuronas conectadas. La «señal» es un número real, y la salida de cada neurona se calcula mediante una función no lineal de la suma de sus entradas, llamada función de activación. La fuerza de la señal en cada conexión está determinada por un peso, que se ajusta durante el proceso de aprendizaje.   Por lo general, las neuronas se agrupan en capas. Las diferentes capas pueden realizar diferentes transformaciones en sus entradas. Las señales viajan desde la primera capa (la capa de entrada) hasta la última capa (la capa de salida), posiblemente pasando por múltiples capas intermedias (capas ocultas). Una red se denomina típicamente red neuronal profunda si tiene al menos dos capas ocultas.    Las redes neuronales se entrenan típicamente a través de la minimización de riesgos empíricos. Este método se basa en la idea de optimizar los parámetros de la red para minimizar la diferencia, o riesgo empírico, entre el resultado previsto y los valores objetivo reales en un conjunto de datos determinado. Los métodos basados en gradientes, como la Retropropagacion o Propagación hacía atrás, se utilizan generalmente para estimar los parámetros de la red.  Durante la fase de entrenamiento, las ANN aprenden de los datos de entrenamiento etiquetados, actualizando iterativamente sus parámetros para minimizar una Función de pérdida definida.  Las redes neuronales artificiales se utilizan para diversas tareas, como el modelado predictivo, el control adaptativo y la resolución de problemas en el ámbito de la inteligencia artificial. Pueden aprender de la experiencia y extraer conclusiones de un conjunto de información complejo y aparentemente no relacionado. Sobresalen en áreas donde la detección de soluciones o características es difícil de expresar con la programación convencional. Para realizar este aprendizaje automático, normalmente, se intenta minimizar una función de pérdida que evalúa la red en su total. Los valores de los pesos de las neuronas se van actualizando buscando reducir el valor de la función de pérdida. Este proceso se realiza mediante la propagación hacia atrás. Las redes neuronales actuales suelen contener desde unos miles a unos pocos millones de unidades neuronales. Nuevas investigaciones sobre el cerebro a menudo estimulan la creación de nuevos patrones en las redes neuronales[cita requerida]. Un nuevo enfoque [cita requerida] está utilizando conexiones que se extienden mucho más allá y capas de procesamiento de enlace en lugar de estar siempre localizado en las neuronas adyacentes. Otra investigación[cita requerida] está estudiando los diferentes tipos de señal en el tiempo que los axones se propagan, como el aprendizaje profundo, interpola una mayor complejidad que un conjunto de variables booleanas que son simplemente encendido o apagado. Las redes neuronales se han utilizado para resolver una amplia variedad de tareas, como la visión por computador y el reconocimiento de voz, que son difíciles de resolver usando la ordinaria programación basada en reglas. Históricamente, el uso de modelos de redes neuronales marcó un cambio de dirección a finales de los años ochenta de alto nivel, que se caracteriza por sistemas expertos con conocimiento incorporado en si-entonces las reglas, a bajo nivel de aprendizaje automático, caracterizado por el conocimiento incorporado en los parámetros de un modelo cognitivo con algún sistema dinámico. Warren McCulloc	Las redes neuronales artificiales (RNA) son modelos de aprendizaje automático inspirados en el cerebro, formados por neuronas interconectadas que procesan información a través de pesos ajustables.  El entrenamiento de las RNA implica minimizar una función de pérdida para optimizar los pesos, permitiendo que aprendan patrones complejos de datos y realicen tareas como el reconocimiento de imágenes y voz.
"Un algoritmo es una serie de pasos organizados que describe el proceso que se debe seguir, para dar solución a un problema específico. En los años 1970, de la mano de John Henry Holland, surgió una de las líneas más prometedoras de la inteligencia artificial, la de los algoritmos genéticos, (AG).​​ Son llamados así porque se inspiran en la evolución biológica y su base genético-molecular.  Estos algoritmos hacen evolucionar una población de individuos sometiéndola a acciones aleatorias, semejantes a las que actúan en la evolución biológica (mutaciones y recombinaciones genéticas), así como también a una selección. De acuerdo con algún criterio, se decide cuáles son los individuos más adaptados, que sobreviven, y cuáles son los menos aptos, que son descartados. Los algoritmos genéticos se enmarcan dentro de los algoritmos evolutivos, que incluyen también las estrategias evolutivas, la programación evolutiva y la programación genética. Los algoritmos genéticos (AG) funcionan entre el conjunto de soluciones de un problema llamado fenotipo, y el conjunto de individuos de una población natural, codificando la información de cada solución en una cadena, generalmente binaria, llamada cromosoma. Los símbolos que forman la cadena son llamados genes. Cuando la representación de los cromosomas se hace con cadenas de dígitos binarios se le conoce como genotipo. Los cromosomas evolucionan a través de iteraciones, llamadas generaciones. En cada generación, los cromosomas son evaluados usando alguna medida de aptitud. Las siguientes generaciones (nuevos cromosomas), son generadas aplicando los operadores genéticos repetidamente, siendo estos los operadores de selección, cruzamiento, mutación y reemplazo. Los algoritmos genéticos son de probada eficacia en caso de querer calcular funciones no derivables (o de derivación muy compleja) aunque su uso es posible con cualquier función. Deben tenerse en cuenta también las siguientes consideraciones: Si la función a optimizar tiene muchos máximos/mínimos locales se requerirán más iteraciones del algoritmo para ""asegurar"" el máximo/mínimo global. Si la función a optimizar contiene varios puntos muy cercanos en valor al óptimo, solamente podemos ""asegurar"" que encontraremos uno de ellos (no necesariamente el óptimo). Un algoritmo genético puede presentar diversas variaciones, dependiendo de cómo se decide el reemplazo de los individuos para formar la nueva población. En general, el pseudocódigo consiste de los siguientes pasos: Inicialización: Se genera aleatoriamente la población inicial, que está constituida por un conjunto de cromosomas los cuales representan las posibles soluciones del problema. En caso de no hacerlo aleatoriamente, es importante garantizar que dentro de la población inicial, se tenga la diversidad estructural de estas soluciones para tener una representación de la mayor parte de la población posible o al menos evitar la convergencia prematura. Evaluación: A cada uno de los cromosomas de esta población se aplicará la función de aptitud para saber cómo de ""buena"" es la solución que se está codificando. Condición de término: El AG se deberá detener cuando se alcance la solución óptima, pero esta generalmente se desconoce, por lo que se deben utilizar otros criterios de detención. Normalmente se usan dos criterios: correr el AG un número máximo de iteraciones (generaciones) o detenerlo cuando no haya cambios en la población. Mientras no se cumpla la condición de término se hace lo siguiente: Selección: Después de saber la aptitud de cada cromosoma se procede a elegir los cromosomas que serán cruzados en la siguiente generación. Los cromosomas con mejor aptitud tienen mayor probabilidad de ser seleccionados. Recombinación o cruzamiento: La recombinación es el principal operador genético, representa la reproducción sexual, opera sobre dos cromo"	Los algoritmos genéticos, inspirados en la evolución biológica, utilizan la selección, mutación y recombinación para encontrar soluciones aproximadas a problemas complejos, iterando a través de generaciones de soluciones codificadas como cromosomas.  Su eficacia se basa en la evaluación de la aptitud de cada solución para optimizar funciones, aunque la convergencia al óptimo global no está garantizada.
La teoría cuántica de campos es una disciplina de la física que aplica los principios de la mecánica cuántica a los sistemas clásicos de campos continuos, por ejemplo, el campo electromagnético. Una consecuencia inmediata de esta teoría es que el comportamiento cuántico de un campo continuo es equivalente al de un sistema de partículas[n 1]​ cuyo número no es constante, es decir, que pueden crearse o destruirse.​ También se la denomina teoría de campos cuánticos, TCC[n 2]​ o QFT, sigla en inglés de quantum field theory. Su principal aplicación es la física de altas energías, donde se combina con los postulados de la relatividad especial. En este régimen se usa para estudiar las partículas subatómicas y sus interacciones, y permite explicar fenómenos como la relación entre espín y estadística, la simetría CPT, la existencia de antimateria, etc.​ También es una herramienta habitual en el campo de la física de la materia condensada, donde se utiliza para describir las excitaciones colectivas de sistemas de muchas partículas y entender efectos físicos tales como la superconductividad, la superfluidez o el efecto Hall cuántico.​ En particular, la teoría cuántica del campo electromagnético, conocida como electrodinámica cuántica, fue el primer ejemplo de teoría cuántica de campos que se estudió y es la teoría física probada experimentalmente con mayor precisión.​ Los fundamentos de la teoría de campos cuántica fueron desarrollados entre las décadas de 1920 y 1950 por Dirac, Fock, Pauli, Tomonaga, Schwinger, Feynman y Dyson, entre otros. El desarrollo de la teoría cuántica de campos ocurrió simultáneamente con el de la mecánica cuántica «ordinaria», en un intento de explicar los fenómenos atómicos tomando también en cuenta las leyes de la teoría de la relatividad.​ Entre 1926 y 1928 se desarrollaron los primeros intentos de encontrar una ecuación de onda relativista que describiera el movimiento de una partícula cuántica, debidos a Erwin Schrödinger y a Paul Dirac. Sin embargo, dichas ecuaciones mostraban ciertas inconsistencias. Por otro lado, en 1926 Werner Heisenberg, Pascual Jordan y Max Born profundizaron en el estudio del problema del cuerpo negro: el comportamiento de la radiación electromagnética dentro de una cavidad, en ausencia de partículas cargadas. Esto constituyó el primer ejemplo de una teoría cuántica de campos, en este caso aplicando las reglas de cuantización al campo electromagnético. En sus resultados, la radiación se comportaba como un conjunto de partículas —los fotones—, en consonancia con la hipótesis de los cuantos de luz, formulada por Einstein en 1905. Tras este ejemplo, las mencionadas ecuaciones de onda relativistas se estudiaron de nuevo desde otro punto de vista. En lugar de interpretarlas como funciones de onda, se usaron las reglas de cuantización de un campo clásico para manipularlas. De este modo se obtuvieron ecuaciones para partículas cuánticas respetando las leyes de la relatividad que sí eran consistentes. Esta reinterpretación, conocida como segunda cuantización, fue llevada a cabo por Heisenberg, Wolfgang Pauli, Vladimir Fock, Wendell Furry, Robert Oppenheimer y Victor Weisskopf. A pesar de sus éxitos iniciales, la teoría cuántica de campos tenía problemas teóricos muy serios. El cálculo de muchas cantidades físicas en apariencia ordinarias resultaba en un valor infinito, un resultado sin sentido. Un ejemplo de esto eran las pequeñas diferencias entre algunos niveles de energía en el átomo de hidrógeno, la llamada estructura fina. Este «problema de las divergencias» fue resuelto durante las décadas de 1930 y 1940 por Julian Schwinger, Freeman Dyson, Richard Feynman y Shin'ichiro Tomonaga entre otros, mediante una técnica conocida como renormalización. Esta etapa culminó con el desarrollo de la moderna electrodinámica cuántica —QED, por Quantum Electrodynamics—. La técnica de los diagramas de Feynman, un procedimiento gráfico de cálculo desarrollado por Richard	La teoría cuántica de campos (TCC) aplica la mecánica cuántica a campos continuos, permitiendo la creación y destrucción de partículas.  Esta teoría, desarrollada entre las décadas de 1920 y 1950, es fundamental en física de altas energías y materia condensada, y ha sido probada con gran precisión experimental.
"En termodinámica, la entropía (simbolizada como S) es una magnitud física para un sistema termodinámico en equilibrio. Mide el número de microestados compatibles con el macroestado de equilibrio; también se puede decir que mide el grado de organización del sistema, o que es la razón de un incremento entre energía interna frente a un incremento de temperatura del sistema termodinámico. La entropía es una función de estado de carácter extensivo y su valor, en un sistema aislado, crece en el transcurso de un proceso que se da de forma natural. La entropía describe lo irreversible de los sistemas termodinámicos. La palabra «entropía» procede del griego (ἐντροπία) y significa evolución o transformación. Fue Rudolf Clausius quien le dio nombre y la desarrolló durante la década de 1850;​​ y Ludwig Boltzmann, quien encontró en 1877 la manera de expresar matemáticamente este concepto, desde el punto de vista de la probabilidad.​ Cuando se plantea la pregunta «¿por qué ocurren los sucesos en la naturaleza de una manera determinada y no de otra manera?», se busca una respuesta que indique cuál es el sentido de los sucesos. Por ejemplo, si se ponen en contacto dos trozos de metal con distinta temperatura, se anticipa que finalmente el trozo caliente se enfriará y el trozo frío se calentará, finalizando en equilibrio térmico. El proceso inverso, el calentamiento del trozo caliente y el enfriamiento del trozo frío es muy improbable que se presente, a pesar de conservar la energía. El universo tiende a distribuir la energía uniformemente; es decir, a maximizar la entropía. Intuitivamente, la entropía es una magnitud física que, mediante cálculo, permite determinar la parte de la energía por unidad de temperatura que no puede utilizarse para producir trabajo. La función termodinámica entropía es central para el segundo principio de la termodinámica. La entropía puede interpretarse como una medida de la distribución aleatoria de un sistema. Se dice que un sistema altamente distribuido al azar tiene alta entropía. Un sistema en una condición improbable tendrá una tendencia natural a reorganizarse hacia una condición más probable (similar a una distribución al azar), reorganización que dará como resultado un aumento de la entropía. La entropía alcanzará un máximo cuando el sistema se acerque al equilibrio, y entonces se alcanzará la configuración de mayor probabilidad. Una magnitud  es una función de estado si, y solo si, su cambio de valor entre dos estados es independiente del proceso seguido para llegar de un estado a otro. Esa caracterización de función de estado es fundamental a la hora de definir la variación de entropía. La variación de entropía nos muestra la variación del orden molecular ocurrido en una reacción química. Si el incremento de entropía es positivo, los productos presentan un mayor desorden molecular (mayor entropía) que los reactivos. En cambio, cuando el incremento es negativo, los productos son más ordenados. Entre la entropía y la espontaneidad de una reacción química se establece una relación que viene dada por la energía de Gibbs. La RAE recoge el concepto de entropía definido como la ""magnitud termodinámica que mide la parte de la energía no utilizable para realizar trabajo y que se expresa como el cociente entre el calor cedido por un cuerpo y su temperatura absoluta.""​ Dentro de la termodinámica o rama de la física que estudia los procesos que surgen a partir del intercambio de energías y de la puesta en movimiento de diferentes elementos naturales, la entropía figura como una especie de desorden de todo aquello que es sistematizado, es decir, como la referencia o la demostración de que cuando algo no es controlado puede transformarse y desordenarse. La entropía, además, supone que de ese caos o desorden existente en un sistema surja una situación de equilibrio u homogeneidad que, a pesar de ser diferente a la condición inicial, suponga q"	La entropía es una magnitud termodinámica que mide el desorden o la aleatoriedad de un sistema, indicando la parte de energía no utilizable para trabajo.  En sistemas aislados, la entropía tiende a aumentar con el tiempo,  moviéndose hacia un estado de equilibrio de máxima probabilidad.
La teoría del caos es la rama de la matemática, la física y otras ciencias (biología, meteorología, entre ellas) que trata ciertos tipos de sistemas complejos y sistemas dinámicos no lineales muy sensibles a las variaciones en las condiciones iniciales. Pequeñas variaciones en dichas condiciones iniciales pueden implicar grandes diferencias en el comportamiento futuro, imposibilitando la predicción a largo plazo. Esto sucede aunque estos sistemas son en rigor deterministas, es decir, su comportamiento puede ser completamente determinado conociendo sus condiciones iniciales. La disciplina asociada a los sistemas caóticos aclara por qué existe cierto tipo de sistemas cuyo comportamiento es prácticamente imposible de predecir, no sólo porque sean complejos formados por muchos elementos, sino también aclara por qué sistemas relativamente simples y con pocos grados de libertad pueden ser difíciles de predecir a largo plazo. El comportamiento caótico existe en muchos sistemas naturales, incluyendo el flujo de fluidos, las irregularidades del ritmo cardíaco, el clima y el tiempo.​ También ocurre espontáneamente en algunos sistemas con componentes artificiales, como el tráfico vehicular. Este comportamiento puede estudiarse mediante el análisis de un modelo matemático caótico o mediante técnicas analíticas como los diagramas de recurrencia y los mapas de Poincaré. La teoría del caos tiene aplicaciones en diversas disciplinas, incluyendo la meteorología, la antropología,​ la sociología, la ciencia ambiental, la informática, la ingeniería, la economía, la ecología y la gestión de crisis pandémicas.​​ La teoría formó la base para campos de estudio como los sistemas complejos, la teoría del borde del caos y los procesos de autoensamblaje. Los sistemas dinámicos se pueden clasificar Estables, cuando dos soluciones con condiciones iniciales suficientemente cercanas siguen siendo cercanas a lo largo del tiempo. Así, un sistema estable tiende a lo largo del tiempo a un punto, u órbita, según su dimensión (atractor o sumidero). Inestables, cuando dos soluciones con condiciones iniciales diferentes acaban divergiendo por pequeñas que sean las diferencias entre las condiciones iniciales. Así un sistema inestable «escapa» de los atractores. Caóticos, cuando el sistema no es inestable y si bien dos soluciones se mantienen a una distancia «finita» cercana a un atractor del sistema dinámico, las soluciones se mueven en torno al atractor de manera irregular y pasado el tiempo ambas soluciones no son cercanas, si bien suelen ser cualitativamente similares. De esa manera, el sistema permanece confinado en una zona de su espacio de estados, pero sin tender a un atractor fijo. Una de las principales características tanto de los sistemas inestables como los caóticos es que tienen una gran dependencia de las condiciones iniciales (esto diferencia a ambos tipos de los sistemas estables). De un sistema del que se conocen sus ecuaciones de evolución temporal características, y con unas condiciones iniciales fijas, se puede conocer exactamente su evolución en el tiempo. Pero en el caso de los sistemas caóticos, una mínima diferencia en esas condiciones hace que el sistema evolucione de manera totalmente distinta. Ejemplos de tales sistemas son el Sistema Solar, las placas tectónicas, los fluidos en régimen turbulento y los incrementos de población.​ El caos determinista comprende una serie de fenómenos encontrados en la teoría de sistemas dinámicos, la teoría de ecuaciones diferenciales y la mecánica clásica. En términos generales el caos determinista da lugar a trayectorias asociadas a la evolución temporal de forma muy irregular y aparentemente azarosa que sin embargo son totalmente deterministas, a diferencia del azar genuino. La irregularidad de las trayectorias está asociada a la imposibilidad práctica de predecir la evolución futura del sistema, aunque esta evolución sea tota	La teoría del caos estudia sistemas dinámicos no lineales altamente sensibles a variaciones iniciales, haciendo impredecibles sus comportamientos a largo plazo a pesar de ser deterministas.  Sus aplicaciones abarcan diversas disciplinas, desde la meteorología hasta la economía, modelando fenómenos complejos con comportamientos aparentemente aleatorios pero regidos por leyes deterministas.
Las células madre o células troncales son células que se encuentran en todos los organismos pluricelulares​ y que tienen la capacidad de dividirse (a través de la mitosis) y diferenciarse en diversos tipos de células especializadas, además de autorrenovarse para producir más células madre.​ En los mamíferos, existen diversos tipos de células madre que se pueden clasificar teniendo en cuenta su potencia celular,​ es decir, el número de diferentes tipos celulares en los que puede diferenciarse.​ En los organismos adultos, las células madre y las células progenitoras actúan en la regeneración o reparación de los tejidos del organismo.​ Las células madre —en inglés stem cells (donde stem significa tronco, traduciéndose a menudo como «células troncales»)— tienen la capacidad de dividirse asimétricamente dando lugar a dos células hijas, una de las cuales tiene las mismas propiedades que la célula madre original (autorrenovación) y la otra adquiere la capacidad de poder diferenciarse si las condiciones ambientales son adecuadas.​ La mayoría de los tejidos de un organismo adulto, poseen una población residente de células madre adultas que permiten su renovación periódica o su regeneración cuando se produce algún daño tisular.​ Algunas células madre adultas son capaces de diferenciarse en más de un tipo celular como las células madre mesenquimales y las células madre hematopoyéticas, mientras que otras son precursoras directas de las células del tejido en el que se encuentran, como por ejemplo las células madre de la piel, músculo intestino o las células madre gonadales (células madre germinales).​ Las células madre embrionarias son aquellas que forman parte de la masa celular interna de un embrión de 4-5 días de edad. Estas son pluripotentes lo cual significa que pueden dar origen a las tres capas germinales: ectodermo, mesodermo y endodermo. Una característica fundamental de las células madre embrionarias es que pueden mantenerse (en el embrión o en determinadas condiciones de cultivo) de forma indefinida, formando al dividirse una célula idéntica a ellas mismas, y manteniendo una población estable de células madre. Existen técnicas experimentales donde se pueden obtener células madre embrionarias sin que esto implique la destrucción del embrión.​ Teniendo en cuenta su potencia,​ las células madre pueden clasificarse en seis tipos:​ Pueden crecer y formar un organismo completo, tanto los componentes embrionarios (como por ejemplo, las tres capas embrionarias, el linaje germinal y los tejidos que darán lugar al saco vitelino), como los extraembrionarios (como la placenta). Es decir, pueden formar todos los tipos celulares.​​ La célula madre totipotente por excelencia es el cigoto, formado cuando un óvulo es fecundado por un espermatozoide. No pueden formar un organismo completo, pero sí cualquier otro tipo de célula correspondiente a los tres linajes embrionarios (endodermo, ectodermo y mesodermo). Pueden, por lo tanto, formar linajes celulares. Se encuentran en distintas etapas del desarrollo embrionario. Las células madre pluripotentes más estudiadas son las células madre embrionarias (en inglés embryonic stem cells o ES cells) que se pueden aislar de la masa celular interna del blastocisto.​ El blastocisto está formado por una capa externa denominada trofoblasto, formada por unas 70 células, y una masa celular interna constituida por unas 30 células que son las células madre embrionarias que tienen la capacidad de diferenciarse en todos los tipos celulares que aparecen en el organismo adulto, dando lugar a los tejidos y órganos. En la actualidad se utilizan como modelo para estudiar el desarrollo embrionario y para entender cuáles son los mecanismos y las señales que permiten a una célula pluripotente llegar a formar cualquier célula plenamente diferen	Las células madre son células capaces de dividirse y diferenciarse en diversos tipos celulares, además de autorrenovarse.  Se clasifican según su potencia, que determina el número de tipos celulares en los que pueden diferenciarse, incluyendo células totipotentes, pluripotentes y multipotentes.
La fotosíntesis o función clorofílica es un proceso químico que consiste en la conversión de materia inorgánica a materia orgánica gracias a la energía que aporta la luz solar. En este proceso, la energía lumínica se transforma en energía química estable, siendo el NADPH (nicotín adenín dinucleótido fosfato) y el ATP (adenosín trifosfato) las primeras moléculas en las que queda almacenada esta energía química. Con posterioridad, el poder reductor del NADPH y el potencial energético del grupo fosfato del ATP se usan para la síntesis de hidratos de carbono a partir de la reducción del dióxido de carbono (CO2). La vida en nuestro planeta se mantiene fundamentalmente gracias a la síntesis que realizan en el medio acuático las algas, las cianobacterias, las bacterias rojas, las bacterias púrpuras, bacterias verdes del azufre,​ y en el medio terrestre las plantas, que tienen la capacidad de sintetizar materia orgánica (imprescindible para la constitución de los seres vivos) partiendo de la luz y la materia inorgánica. De hecho, cada año los organismos fotosintetizadores fijan en forma de materia orgánica en torno a 100 000 millones de toneladas de carbono.​​ La vida en la Tierra depende fundamentalmente de la energía solar. Esta energía es atrapada mediante la fotosíntesis, responsable de la producción de toda la materia orgánica de la vida (biomasa). Los orgánulos citoplasmáticos encargados de la realización de la fotosíntesis son los cloroplastos, unas estructuras polimorfas y de color verde (esta coloración es debida a la presencia del pigmento clorofila) propias de las células vegetales. En el interior de estos orgánulos se halla una cámara que alberga un medio interno llamado estroma, que alberga diversos componentes, entre los que cabe destacar enzimas encargadas de la transformación del dióxido de carbono en materia orgánica y unos sáculos aplastados denominados tilacoides, cuya membrana contiene pigmentos fotosintéticos. En términos medios, una célula foliar tiene entre cincuenta y sesenta cloroplastos en su interior.​  Los organismos que tienen la capacidad de llevar a cabo la fotosíntesis son llamados, fotoautótrofos (otra nomenclatura posible es la de autótrofos, pero se debe tener en cuenta que bajo esta denominación también se engloban aquellas bacterias que realizan la quimiosíntesis) y fijan el CO2 atmosférico. En la actualidad se diferencian dos tipos de procesos fotosintéticos, que son la fotosíntesis oxigénica y la fotosíntesis anoxigénica. La primera de las modalidades es la propia de las plantas superiores, las algas y las cianobacterias, donde el dador de electrones es el agua y, como consecuencia, se desprende oxígeno. Mientras que la segunda, también conocida con el nombre de fotosíntesis bacteriana, la realizan las bacterias purpúreas y verdes del azufre, en las que el dador de electrones es el sulfuro de hidrógeno (H2S), y consecuentemente, el elemento químico liberado no será oxígeno sino azufre, que puede ser acumulado en el interior de la bacteria, o en su defecto, expulsado al agua.​ Se han encontrado animales capaces de favorecerse de la fotosíntesis, tales como Elysia chlorotica, una babosa marina con apariencia de hoja, y Ambystoma maculatum, una salamandra.​​​​​ A comienzos del año 2009, se publicó un artículo en la revista científica Nature Geoscience en el que científicos estadounidenses daban a conocer el hallazgo de pequeños cristales de hematita (en el cratón de Pilbara, en el noroeste de Australia), un mineral de hierro datado en el eón Arcaico, reflejando así la existencia de agua rica en oxígeno y, consecuentemente, de organismos fotosintetizadores capaces de producirlo. Según este estudio y atendiendo a la datación más antigua del cratón, la existencia de fotosíntesis oxigénica y la oxigenación de la atmósfera y océanos se habría producido desde hace más de 3.460 millones de años, de lo que se deduciría la existencia de un número considerable de organismos capac	La fotosíntesis convierte la energía solar y la materia inorgánica en materia orgánica, siendo fundamental para la vida en la Tierra al producir biomasa y oxígeno.  Organismos como plantas y algas realizan este proceso, utilizando cloroplastos para transformar dióxido de carbono en hidratos de carbono.
"La teoría general de la relatividad o relatividad general es una teoría del campo gravitatorio y de los sistemas de referencia generales, publicada por Albert Einstein en 1915 y 1916. El nombre de la teoría se debe a que generaliza la llamada teoría especial de la relatividad y el principio de relatividad para un observador arbitrario. Los principios fundamentales introducidos en esta generalización son el principio de equivalencia, que describe la aceleración y la gravedad como aspectos distintos de la misma realidad, la noción de la curvatura del espacio-tiempo y el principio de covariancia generalizado. La teoría de la relatividad general propone que la propia geometría del espacio-tiempo se ve afectada por la presencia de materia, de lo cual resulta una teoría relativista del campo gravitatorio. De hecho la teoría de la relatividad general predice que el espacio-tiempo no será plano en presencia de materia y que la curvatura del espacio-tiempo será percibida como un campo gravitatorio. La intuición básica de Einstein fue postular que en un punto concreto no se puede distinguir experimentalmente entre un cuerpo acelerado uniformemente y un campo gravitatorio uniforme. La teoría general de la relatividad permitió también reformular el campo de la cosmología. Einstein expresó el propósito de la teoría de la relatividad general para aplicar plenamente el programa de Ernst Mach de la relativización de todos los efectos de inercia, incluso añadiendo la llamada constante cosmológica a sus ecuaciones de campo​ para este propósito. Este punto de contacto real de la influencia de Ernst Mach fue claramente identificado en 1918, cuando Einstein distingue lo que él bautizó como el principio de Mach (los efectos inerciales se derivan de la interacción de los cuerpos) del principio de la relatividad general, que se interpreta ahora como el principio de covariancia general.​ El matemático alemán David Hilbert escribió e hizo públicas las ecuaciones de la covariancia antes que Einstein, ello resultó en no pocas acusaciones de plagio contra Einstein, pero probablemente sea más porque es una teoría (o perspectiva) geométrica. La misma postula que la presencia de masa o energía «curva» el espacio-tiempo, y esta curvatura afecta la trayectoria de los cuerpos móviles e incluso la trayectoria de la luz. Poco después de la formulación de la teoría de la relatividad especial en 1905, Albert Einstein comenzó a elucubrar cómo describir los fenómenos gravitatorios con ayuda de la nueva mecánica. En 1907 se embarcó en la búsqueda de una nueva teoría relativista de la gravedad que duraría ocho años. Después de numerosos desvíos y falsos comienzos, su trabajo culminó el 25 de noviembre de 1915 con la presentación a la Academia Prusiana de las Ciencias de su artículo, que contenía las que hoy son conocidas como ""Ecuaciones de Campo de Einstein"". Estas ecuaciones forman el núcleo de la teoría y especifican cómo la densidad local de materia y energía determina la geometría del espacio-tiempo. Las ecuaciones de campo de Einstein son no lineales y muy difíciles de resolver. Einstein utilizó los métodos de aproximación en la elaboración de las predicciones iniciales de la teoría. Pero ya en 1916, el astrofísico Karl Schwarzschild encontró la primera solución exacta no trivial de las Ecuaciones de Campo de Einstein, la llamada Métrica de Schwarzschild. Esta solución sentó las bases para la descripción de las etapas finales de un colapso gravitacional, y los objetos que hoy conocemos como agujeros negros. En el mismo año, se iniciaron los primeros pasos hacia la generalización de la solución de Schwarzschild a los objetos con carga eléctrica, obteniéndose así la solución de Reissner-Nordström, ahora asociada con la carga eléctrica de los agujeros negros. En 1917, Einstein aplicó su teoría al universo en su conjunto, iniciando el campo de la cosmología relativista. En línea con el pensamiento contemporáneo, en el que se suponía que el univ"	La teoría general de la relatividad de Einstein, publicada en 1915-1916, explica la gravedad como la curvatura del espacio-tiempo causada por la masa y la energía, generalizando la relatividad especial.  Esta teoría revolucionó la cosmología y predijo fenómenos como los agujeros negros.
La nanotecnología es la manipulación de la materia a una escala nanométrica. La más temprana descripción de la nanotecnología​​ se refiere a la meta tecnológica particular de manipular en forma precisa los átomos y moléculas para la fabricación de productos con componentes muy pequeños (a nanoescala) donde se hacen visibles los efectos de la mecánica cuántica, ahora también referida como nanotecnología molecular. La Iniciativa Nanotecnológica Nacional de Estados Unidos, define de forma más general la nanotecnología como la manipulación de la materia con al menos una dimensión del tamaño de entre 1 a 100 nanómetros. Esta definición es ampliamente aceptada en el campo de la física y la química; sin embargo en el campo de la biología se aceptan como nanopartículas aquellas con un al menos una dimensión menor a 1000 nanómetros. Esta definición refleja el hecho de que los efectos de la mecánica cuántica son importantes a esta escala del dominio cuántico y, así, la definición pasó de referirse a una tecnología en particular a una campo más amplio que incluye todos los tipos de investigación y tecnologías relacionadas con las propiedades especiales de la materia bajo cierto umbral de tamaño. Es común el uso de la forma plural «nanotecnologías» así como «tecnologías de nanoescala». Debido a la variedad de potenciales aplicaciones médicas, industriales y militares, los gobiernos han invertido miles de millones de dólares en investigación de la nanotecnología. A través de su Iniciativa Nanotecnológica Nacional, Estados Unidos ha invertido 3700 millones de dólares. La Unión Europea ha invertido[cita requerida] 1200 millones y Japón 750 millones de dólares.​ La nanotecnología definida por el tamaño es naturalmente un campo muy amplio, que incluye diferentes disciplinas de la ciencia tan diversas como la ciencia de superficies, química orgánica, biología molecular, física de los semiconductores, microfabricación, etc.​ Las investigaciones y aplicaciones asociadas son igualmente diversas, yendo desde extensiones de la física de los dispositivos a nuevas aproximaciones completamente nuevas basadas en el autoensamblaje molecular, desde el desarrollo de nuevos materiales con dimensiones en las nanoescalas al control directo de la materia a escala atómica. Actualmente los científicos están debatiendo el futuro de las implicaciones de la nanotecnología. La nanotecnología puede ser capaz de crear nuevos materiales y dispositivos con un vasto alcance de aplicaciones, tales como en la medicina, electrónica, biomateriales, y la producción de energía. Por otra parte, la nanotecnología hace surgir las mismas preocupaciones que cualquier nueva tecnología, incluyendo preocupaciones acerca de la toxicidad y el impacto ambiental de los nanomateriales,​ y sus potenciales efectos en la economía global, así como especulaciones acerca de varios escenarios apocalípticos. Estas preocupaciones han llevado al debate entre varios grupos de defensa y gobiernos sobre si se requieren regulaciones especiales para la nanotecnología. No existe ni unanimidad ni claridad en torno al uso de la nanotecnología y todo de lo que ella se deriva: tanto la seguridad como la privacidad de los posibles riesgos no están calculados. ​​​ La nanotecnología comprende el estudio, diseño, creación, síntesis, manipulación y aplicación de materiales, aparatos y sistemas funcionales a través del control de la materia a nanoescala, y la explotación de fenómenos y propiedades de la materia a nanoescala.  Cuando se manipula la materia a escala tan minúscula, presenta fenómenos y propiedades totalmente nuevas. Por lo tanto, los científicos utilizan la nanotecnología para crear materiales, aparatos y sistemas novedosos y poco costosos con propiedades únicas. No obstante, la nanociencia es una disciplina dedicada al estudio de los fenómenos físicos, químicos y biológicos que ocurren a escala nanométr	La nanotecnología implica la manipulación de la materia a escala nanométrica, aprovechando propiedades únicas a ese nivel.  Esta área interdisciplinaria, con grandes inversiones gubernamentales, presenta un enorme potencial de aplicaciones pero también plantea preocupaciones sobre seguridad y medio ambiente.
